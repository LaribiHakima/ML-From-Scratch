{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 02 : Régression logistique Multinomiale\n",
    "\n",
    "Binômes : \n",
    "- Ould Taleb Nada\n",
    "- Laribi Hakima\n",
    "\n",
    "\n",
    "**INTRODUCTION**\n",
    "\n",
    "Nous avons implémenté le cas d'une seule classe (binaire : oui ou non). Pour appliquer un classement sur plusieurs classes, on peut entrainner $L$ modèles de régression logistique (où $L$ est le nombre des classes). Dans ce cas, nos résultats (Y) doivent encodée en 0 et 1. Pour un modèle $M_i$ d'une classe $C_i$, la sortie $Y$ doit avoir 1 si $C_i$, 0 si une autre classe. (One-to-rest classification)\n",
    "\n",
    "Une autre approche (celle qu'on va implémenter) est d'encoder la sortie en utilisant OneHot encoder. Pour $L$ classes et un échantillon donnée, on va avoir $L$ sorties (une ayant 1 et les autres 0). Pour un dataset avec $M$ échantillons, $N$ caractéristiques et $L$ classes, on va avoir les dimensions suivantes : \n",
    "- $X [M, N]$\n",
    "- $Y [M, L]$\n",
    "- $\\theta [N, L]$\n",
    "\n",
    "Cette dernière approche s'appelle maximum entropy (MaxEnt). Elle généralise la régresion logistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import outils\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Réalisation des algorithmes\n",
    "\n",
    "Cette partie sert à améliorer la compréhension les algorithmes d'apprentissage automatique vus en cours en les implémentant à partir de zéro. \n",
    "Pour ce faire, on va utiliser la bibliothèque **numpy** qui est utile dans les calcules surtout matricielles.\n",
    "\n",
    "### I.1. Combinaison linéaire\n",
    "\n",
    "On combine les $N$  caractéristiques linéairement comme dans la régression linéaire binaire. \n",
    "La seule différence est que nous avons plus de classes, donc le nombre des paramètres va être multiplié par le nombre des classes.\n",
    "La somme pondérée d'une classe $c$ est calculée selon la formule : \n",
    "\n",
    "$$Z_c = zfn_c(X, \\theta) = \\sum\\limits_{j=0}^{N} \\theta_{(c, j)} X_j | X_0 = 1 $$\n",
    "\n",
    "La forme matricielle de $Z$ sera : \n",
    "$$Z = zfn(X, \\theta) = X \\cdot \\theta$$\n",
    "\n",
    "- $X[M, N]$ : une matrice de M lignes (échantillons) et N colonnes (caractéristiques, y compris le biais).  \n",
    "- $\\theta[N, L]$ : une matrice de N lignes (caractéristiques, y compris le biais) et L colonnes (classes). \n",
    "- $Z[M, L]$ : une matrice de M lignes (échantillons) et L colonnes (classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0. ],\n",
       "       [0.5, 0.1, 0.6],\n",
       "       [0.2, 0.3, 0. ],\n",
       "       [0.7, 0.4, 0.6]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO implémenter la fonction de combinaison linéaire \n",
    "def zfn(X, Theta): \n",
    "    return X.dot(Theta)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[0. , 0. , 0. ],\n",
    "#        [0.5, 0.1, 0.6],\n",
    "#        [0.2, 0.3, 0. ],\n",
    "#        [0.7, 0.4, 0.6]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "X_tn = np.array([[0., 0.], \n",
    "                 [1., 0.], \n",
    "                 [0., 1.], \n",
    "                 [1., 1.]]) # 4 échntillons, 2 caractéristiques\n",
    "Theta_tn = np.array([[0.5, 0.1, 0.6],\n",
    "                     [0.2, 0.3, 0.0]]) # 2 caractéristiques, 3 classes\n",
    "zfn(X_tn, Theta_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Calcul des probabilités\n",
    "\n",
    "Les valeurs combinées sont transformées à des probabilités en utilisant la fonction softmax. \n",
    "La fonction softmax nous assure que la somme des probabilités des classes égale à 1.\n",
    "Cette fonction prend les combinaisons linéaires $Z[M, L]$ et calcule les probabilités $P[M, L] comme suite : \n",
    "\n",
    "$$softmax(Z)=\\frac{e^Z}{\\sum\\limits_{k=1}^{L} e^{Z_k}}$$\n",
    "\n",
    "- $M$ nombre des échantillons\n",
    "- $N$ nombre des caractéristiques\n",
    "- $L$ nombre des classes\n",
    "- La somme des probabilités de chaque ligne doit être 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.33333333, 0.33333333],\n",
       "       [0.36029662, 0.24151404, 0.39818934],\n",
       "       [0.34200877, 0.37797814, 0.28001309],\n",
       "       [0.37797814, 0.28001309, 0.34200877]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO compléter la fonction softmax\n",
    "def softmax(Z):\n",
    "    return (np.exp(Z)/np.sum(np.exp(Z),keepdims=True,axis=1))\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[0.33333333, 0.33333333, 0.33333333],\n",
    "#       [0.36029662, 0.24151404, 0.39818934],\n",
    "#       [0.34200877, 0.37797814, 0.28001309],\n",
    "#       [0.37797814, 0.28001309, 0.34200877]])\n",
    "#---------------------------------------------------------------------\n",
    "Z_tn = np.array([[0. , 0. , 0. ],\n",
    "                 [0.5, 0.1, 0.6],\n",
    "                 [0.2, 0.3, 0. ],\n",
    "                 [0.7, 0.4, 0.6]])\n",
    "softmax(Z_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Prédiction \n",
    "\n",
    "Etant donnée les probabilités des classes pour chaque échantillon, on doit choisir la classe avec le max de probabilité.\n",
    "\n",
    "$$\n",
    "\\hat{C}^{(i)}_j = \\begin{cases}\n",
    "1 & si & H^{(i)}_j \\ge \\max P^{(i)} \\\\\n",
    "0 & sinon & \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- $H[M, L]$ probabilités où chaque ligne est yn échantillon et chaque collone est une classe\n",
    "- $\\hat{C}[M, L]$ prédictions où chaque ligne est yn échantillon et chaque collone est une classe. $\\hat{C}^{(i)}_j \\in \\{0, 1\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO compléter la fonction de prédiction \n",
    "# Elle doit calculer la \n",
    "# H est un vecteur de probabilités \n",
    "def cn(H): \n",
    "    K=np.zeros((H.shape[0],H.shape[1]))\n",
    "    for i in range(0,H.shape[0]):\n",
    "        P=np.max(H[i])\n",
    "        K[i,:]=np.where(H[i] == P,1,0)\n",
    "    return K\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[1, 1, 1],\n",
    "#        [0, 0, 1],\n",
    "#        [0, 1, 0],\n",
    "#        [1, 0, 0]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "H_tn = np.array([[0.33333333, 0.33333333, 0.33333333],\n",
    "             [0.36029662, 0.24151404, 0.39818934],\n",
    "             [0.34200877, 0.37797814, 0.28001309],\n",
    "             [0.37797814, 0.28001309, 0.34200877]])\n",
    "cn(H_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4. Calcul du coût \n",
    "\n",
    "On réfère aux probabilités trouvées par la fonction softmax comme $H$, où $H_c$ est la probabilité d'une classe $c$.\n",
    "Etant donné un échantillon $X^{(i)}$, son coût est calculé comme : \n",
    "\n",
    "$$ cout(H^{(i)}, Y^{(i)}) = - \\sum\\limits_{c=1}^{L} Y^{(i)}_c \\log(H^{(i)}_c)$$\n",
    "\n",
    "Le coût total est la moyenne des coût de tous les échantillons\n",
    "\n",
    "$$J(H, Y) = \\frac{1}{M} \\sum\\limits_{i=1}^{M} cout(H^{(i)}, Y^{(i)})$$\n",
    "\n",
    "- $H[M, L]$ : les probabilités estimées de chaque échantillon (M) de chaque classe (L)\n",
    "- $Y[M, L]$ : les probabilités réelles (1 ou 0) de chaque échantillon (M) de chaque classe (L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1913194530574498"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO compléter la fonction du coût multinomial\n",
    "def jn(H, Y): \n",
    "    M=H.shape[0]\n",
    "    return 1/M * np.sum(-Y*np.log(H))\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : 1.1913194530574498\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "H_tn = np.array([[0.33333333, 0.33333333, 0.33333333],\n",
    "                 [0.36029662, 0.24151404, 0.39818934],\n",
    "                 [0.34200877, 0.37797814, 0.28001309],\n",
    "                 [0.37797814, 0.28001309, 0.34200877]])\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n",
    "\n",
    "jn(H_tn, Y_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.5. Calcul des gradients\n",
    "\n",
    "La taille des gradients est la même que celle des paramètres $\\theta[N, L]$. \n",
    "\n",
    "$$\\frac{\\partial J}{\\theta_j} = \\frac{1}{M} \\sum\\limits_{i=1}^{M} (H^{(i)} - Y^{(i)}) X^{(i)}_{j} $$\n",
    "\n",
    "Sa forme matricielle sera \n",
    "$$\\frac{\\partial J}{\\theta_j} = \\frac{1}{M} X^\\top \\cdot (H-Y) $$\n",
    "\n",
    "- $X[M, N]$ : une matrice de M lignes (échantillons) et N colonnes (caractéristiques, y compris le biais).  \n",
    "- $H[M, L]$ : les probabilités estimées de chaque échantillon (M) de chaque classe (L)\n",
    "- $Y[M, L]$ : les probabilités réelles (1 ou 0) de chaque échantillon (M) de chaque classe (L)\n",
    "- $\\frac{\\partial J}{\\theta}[N, L]$ : une matrice de L lignes (classes) et N colonnes (caractéristiques, y compris le biais). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06543131, -0.11961822,  0.18504953],\n",
       "       [-0.07000327,  0.16449781, -0.09449454]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO compléter la fonction du gradient multinomial\n",
    "def dJn(X, H, Y):\n",
    "    M=H.shape[0]\n",
    "    return 1/M * (X.T).dot((H-Y))\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[-0.06543131, -0.11961822,  0.18504953],\n",
    "#        [-0.07000327,  0.16449781, -0.09449454]])\n",
    "#---------------------------------------------------------------------\n",
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]])\n",
    "H_tn = np.array([[0.33333333, 0.33333333, 0.33333333],\n",
    "                 [0.36029662, 0.24151404, 0.39818934],\n",
    "                 [0.34200877, 0.37797814, 0.28001309],\n",
    "                 [0.37797814, 0.28001309, 0.34200877]])\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n",
    "\n",
    "dJn(X_tn, H_tn, Y_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.6. Descente du gradient adaptative\n",
    "\n",
    "Les coéfficients sont mis à jour itérativement en se basant sur le gradient et un taux d'apprentissage $\\alpha$ comme dans la descente des gradients normale. La différence est que dans **AdaGrad**, on adapte le taux d'apprentissage de chaque paramètre $\\theta$ selon l'historique des gradients.\n",
    "\n",
    "\n",
    "#### I.6.1. Mise à jours des gradients\n",
    "\n",
    "Dans cette fonction, on va implémenter la désente des gradients normale (celle dans le TP précédent ... Hint: copy-coller)\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\frac{\\partial J(\\theta)}{\\theta_j}$$\n",
    "\n",
    "aussi, la déscente des gradients adaptative (AdaGrad)\n",
    "\n",
    "$$V = V + (\\frac{\\partial J(\\theta)}{\\partial \\theta})^2$$\n",
    "\n",
    "$$\\theta = \\theta - \\frac{\\alpha}{\\sqrt{V +\\epsilon}} \\frac{\\partial J(\\theta)}{\\theta}$$\n",
    "\n",
    "- $\\theta[N, L]$ : les paramètres de $L$ classes et $N$ caractéristiques\n",
    "- $\\frac{\\partial J(\\theta)}{\\partial \\theta}[N, L]$ les gradients de ces paramètres\n",
    "- $V[N, L]$ : hyper-paramètre pour l'adjustement du taux d'apprentissage pour chaque paramètre\n",
    "- $\\epsilon=e^{-8}$\n",
    "\n",
    "\n",
    "Quelques conditions sur la fonction :\n",
    "- La fonction doit retourner les paramètres mises-à-jours et le nouveau vecteur $V$\n",
    "- Si la valeur booléenne $adagrad=True$, on doit appliquer adagrad (calculer le nouveau V et mettre à jours $\\theta$ selon AdaGrad)\n",
    "- Si cette valeur égale à $False$, on utilise la mise à jour normale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[0.50065431, 0.10119618, 0.5981495 ],\n",
       "         [0.20070003, 0.29835502, 0.00094495]]),\n",
       "  array([[0.2 , 0.1 , 0.5 ],\n",
       "         [0.05, 0.15, 0.2 ]])),\n",
       " (array([[0.50144768, 0.103538  , 0.59746826],\n",
       "         [0.20298765, 0.29609069, 0.00206732]]),\n",
       "  array([[0.20428126, 0.11430852, 0.53424333],\n",
       "         [0.05490046, 0.17705953, 0.20892922]])))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Définir la fonction de la mise à jours des paramètres avec AdaGrad\n",
    "def majThetaAdaGrad(Theta, Gradient, alpha, V, adagrad=False, eps=1e-08): \n",
    "    if (adagrad==False):\n",
    "        return Theta - alpha * Gradient, V\n",
    "    else : \n",
    "        Theta_new = Theta.copy()\n",
    "        V_new = V.copy()\n",
    "        V_new = V_new + Gradient**2\n",
    "        Theta_new = Theta - (alpha / np.sqrt(V_new+eps))* Gradient\n",
    "        return Theta_new, V_new\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# ((array([[0.50065431, 0.10119618, 0.5981495 ],\n",
    "#          [0.20070003, 0.29835502, 0.00094495]]),\n",
    "#   array([[0.2 , 0.1 , 0.5 ],\n",
    "#          [0.05, 0.15, 0.2 ]])),\n",
    "#  (array([[0.50144768, 0.103538  , 0.59746826],\n",
    "#          [0.20298765, 0.29609069, 0.00206732]]),\n",
    "#   array([[0.20428126, 0.11430852, 0.53424333],\n",
    "#          [0.05490046, 0.17705953, 0.20892922]])))\n",
    "#---------------------------------------------------------------------\n",
    "Theta_tn = np.array([[0.5, 0.1, 0.6],\n",
    "                     [0.2, 0.3, 0.0]]) # 2 caractéristiques, 3 classes\n",
    "Gradient_tn = np.array([[-0.06543131, -0.11961822,  0.18504953], \n",
    "                       [-0.07000327,  0.16449781, -0.09449454]])# 2 caractéristiques, 3 classes\n",
    "V_tn = np.array([[0.2, 0.1, 0.5],\n",
    "                [0.05, 0.15, 0.2]]) # 2 caractéristiques, 3 classes\n",
    "alpha_tn = 0.01\n",
    "majThetaAdaGrad(Theta_tn, Gradient_tn, alpha_tn, V_tn), majThetaAdaGrad(Theta_tn, Gradient_tn, alpha_tn, V_tn, adagrad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.6.2. La descente des gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.84298097,  1.57919742, -1.22217839],\n",
       "        [ 0.60036187, -1.45101777,  1.3506559 ]]),\n",
       " 0.5977646913907274)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def des_grad_adagrad(X, Y, Theta, max_iter=200, alpha=0.1, adagrad=False):\n",
    "    \n",
    "    couts = []\n",
    "    \n",
    "    V = np.zeros(Theta.shape) # Générer des zéros\n",
    "    Theta1 = Theta.copy() # pour ne pas modifier Theta original\n",
    "    \n",
    "    for i in range(max_iter): # Ici, la seule condition d'arrêt est le nombre des itérations\n",
    "        H = softmax(zfn(X, Theta1))\n",
    "        couts.append(jn(H, Y))\n",
    "        Theta1, V = majThetaAdaGrad(Theta1, dJn(X, H, Y), alpha, V, adagrad=adagrad)\n",
    "    \n",
    "    return Theta1, couts\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (array([[ 0.84298097,  1.57919742, -1.22217839],\n",
    "#         [ 0.60036187, -1.45101777,  1.3506559 ]]),\n",
    "#  0.5977646913907274)\n",
    "#---------------------------------------------------------------------\n",
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]]) # deux variables logiques\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]]) # égale, sup, inf, égale\n",
    "Theta_tn = np.array([[0.5, 0.1, 0.6],\n",
    "                     [0.2, 0.3, 0.0]]) # 2 caractéristiques, 3 classes\n",
    "\n",
    "theta_n, couts_n = des_grad_adagrad(X_tn, Y_tn, Theta_tn)\n",
    "\n",
    "theta_n, couts_n[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.7. Regrouper les fonctions ensemble \n",
    "\n",
    "Pour bien gérer l'entrainnement et la prédiction, on rassemble les fonctions que vous avez implémenté dans une seul classe. L'intérêt : \n",
    "- Si on applique la normalisation durant l'entrainnement, on doit l'appliquer aussi durant la prédiction. En plus, on doit utiliser les mêmes paramètres (moyenne et écart-type)\n",
    "- On utilise les thétas optimales lors de la prédicition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MaxEnt(object):\n",
    "    \n",
    "    def __init__(self, norm=True, const=True): \n",
    "        self.norm = norm\n",
    "        self.const = const\n",
    "    \n",
    "    def entrainer(self, X, Y, max_iter=100, alpha=.01, adagrad=False): \n",
    "        X_pre, self.mean, self.std = outils.preparer(X, norm=self.norm, const=self.const)\n",
    "        Theta = np.zeros((X_pre.shape[1], Y.shape[1])) # Theta[N, L]\n",
    "        self.Theta, self.couts = des_grad_adagrad(X_pre, Y, Theta, max_iter=max_iter, alpha=alpha, adagrad=adagrad)\n",
    "        \n",
    "        \n",
    "    # La prédiction\n",
    "    # si prob=True elle rend un vecteur de probabilités\n",
    "    # sinon elle rend une vecteur de 1 et 0\n",
    "    def predire(self, X, prob=True):\n",
    "        X_pre, self.mean, self.std = outils.preparer(X, norm=self.norm, const=self.const, mean=self.mean, std=self.std)\n",
    "        H = softmax(zfn(X_pre, self.Theta))\n",
    "        if prob:\n",
    "            return H\n",
    "        return cn(H)\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[1, 0, 0],\n",
    "#        [0, 1, 0],\n",
    "#        [0, 1, 0],\n",
    "#        [0, 0, 1]])\n",
    "#---------------------------------------------------------------------\n",
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]]) # deux variables logiques\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]]) # égale, sup, inf, égale\n",
    "\n",
    "X_testn = np.array([[2., 2.], [1., 0.], [1., -1.], [2., 5.]])\n",
    "\n",
    "maxent = MaxEnt()\n",
    "maxent.entrainer(X_tn, Y_tn)\n",
    "maxent.predire(X_testn, prob=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Application et analyse\n",
    "\n",
    "On va utiliser [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris) pour classer des fleurs en trois classes, en utilisant 4 caractéristiques. Pour simplification, on va utiliser seulement 2 caractéristiques: Petal Length (cm); Petal Width (cm). D'après [Ce tutoriel](https://teddykoker.com/2019/06/multi-class-classification-with-logistic-regression-in-python/) ces 2 caractéristiques sont suffisantes.\n",
    "\n",
    "**Dans cette partie, vous n'avez rien à programmer. Mais, il faut analyser les résultats à la fin**\n",
    "\n",
    "Deux solutions à analyser : \n",
    "- Entrainer 3 modèles de régression logistique binaire\n",
    "- Entrainer 1 modèle de régression logistique multinomial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.read_csv(\"datasets/iris.csv\")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   petal_length  petal_width        class\n",
       "0           1.4          0.2  Iris-setosa\n",
       "1           1.4          0.2  Iris-setosa\n",
       "2           1.3          0.2  Iris-setosa\n",
       "3           1.5          0.2  Iris-setosa\n",
       "4           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if iris.shape[1] > 3:\n",
    "    iris.drop([\"sepal_length\", \"sepal_width\"], axis = 1, inplace=True) #supprimer les colonnes avant la colonne 3\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1. Séparabilité des classes\n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous concernant la séparabilité des 3 classes?\n",
    "- Donner une hypothèse concernant la performance des modèles sur ce dataset (Rappel, Précision)\n",
    "- Justifier cette hypothèse (Rappel, Précision) en comparant les 3 classes\n",
    "\n",
    "**Réponse**\n",
    "- On remarque que la classe rouge est parfaitement séparée des classes bleue et verte, mais les classes verte et bleue sont un peu confondu entre elles (les fleures versicolor et virginica possédent des caractéristiques communes).\n",
    "- Hypothése : pour la classe rouge les deux métriques sont proches de 1, contrairement aux classes bleue et vert qui risquent d'avoir des valeurs de rappel et precision moins proche de 1 donc de moins bonnes prédictions (plus d'erreurs).  \n",
    "- Justification : etant donné que la classe rouge est bien séparée des deux autres classes il ya moins de risques d'avoir de fausses prédictions (des FN et des FP) ce qui fait que le rappel et la precision tonderont plus vers 1. Par contre puisque les deux classes vert et bleue sont confondues entre elles il y'a plus de chance de prédire un vert comme étant un bleue et inversement car ces deux types de fleures possédent des caractéristiques communes (on aura des FN et/ou des FP) et donc le rappel et la précision s'éloigneront plus de 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEHCAYAAABMRSrcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAArlUlEQVR4nO3dfXxU9Zn38c9FMhQjilVwrQiJ9kW3hYQHCVgfi4VVrPjU6tIWvaUqqYlUXddWd71vqtyl7rasslrBpQpqEx9arFarrltUFKtbDTSK4GMtWAp3pbCiGFAI1/3HTGIyOZPMZObM4/f9es0rmTO/c841CcyVc36/6/czd0dEREpbv1wHICIiuadkICIiSgYiIqJkICIiKBmIiAhQnusA+mLw4MFeVVWV6zBERArKqlWr/uruQ4JeK8hkUFVVRXNzc67DEBEpKGa2IdFruk0kIiJKBiIiomQgIiIUaJ9BkN27d7Nx40Z27dqV61CkkwEDBnDYYYcRiURyHYqI9KBoksHGjRvZb7/9qKqqwsxyHY4A7s7WrVvZuHEjhx9+eK7DEZEehHqbyMyGmdlTZvaqma01s8sC2kwys+1m1hJ7zOnLuXbt2sVBBx2kRJBHzIyDDjpIV2uSUU1rmqhaUEW/6/pRtaCKpjVNae2bzvGKSdhXBnuAf3T31Wa2H7DKzH7j7uvi2q1092npnkyJIP/odyKZ1LSmibqH62jd3QrAhu0bqHu4DoAZNTNS3vdbD34LM+Pjto9TPl6xCfXKwN03u/vq2PcfAK8CQ8M8p4gUr2ueuKbjw7xd6+5Wrnnimj7tu3vv7o5EkOrxik3WRhOZWRUwDvhdwMtHm9lLZvaYmY1KsH+dmTWbWfOWLVvCDLXPBg4cmPC1Y445JrTz/vCHPwzt2CL55J3t76S0PdU2fWlbLLKSDMxsIHA/cLm7vx/38mqg0t3HADcDDwYdw90Xu3utu9cOGRJYTZ2X2traAHjuuedCO4eSgZSK4YOGp7Q91TZ9aVssQk8GZhYhmgia3P2X8a+7+/vuviP2/aNAxMwGhx0XTU1QVQX9+kW/NmWu02jFihWceOKJfPOb36Smpgb45Kph8+bNnHDCCYwdO5bq6mpWrlzZbf+1a9cyceJExo4dy+jRo3nzzTcBaGxs7Nj+7W9/m7a2Nq6++mp27tzJ2LFjmTEjeo/zhhtuoLq6murqahYsWADAhx9+yKmnnsqYMWOorq7mvvvuA2Du3LlMmDCB6upq6urq0Mp3ks/mTZ5HRaSiy7aKSAXzJs/r076RfhH6l/Xv0/GKjruH9gAMuAtY0EObQwCLfT8ReKf9eaLH+PHjPd66deu6bUuosdG9osIdPnlUVES3p2Hfffd1d/ennnrKKyoq/O233+722vz58/0HP/iBu7vv2bPH33///W7HmT17tjfGYvnoo4+8tbXV161b59OmTfOPP/7Y3d3r6+v9zjvv7HJsd/fm5mavrq72HTt2+AcffOAjR4701atX+7Jly/yiiy7qaPfee++5u/vWrVs7tp177rn+0EMPpfUzCJLS70akF40vN3rljZVu15pX3ljpjS8n//82aN90jldogGZP8Lka9miiY4HzgDVm1hLb9s/A8FgiuhU4G6g3sz3ATuDrsaDDc8010Nq1I4nW1uj2GZkZQTBx4sTAsfUTJkzgggsuYPfu3Zx55pmMHTu2W5ujjz6aefPmsXHjRr761a8yYsQInnjiCVatWsWECRMA2LlzJwcffHC3fZ999lnOOuss9t13XwC++tWvsnLlSqZOncqVV17JVVddxbRp0zj++OMBeOqpp/jRj35Ea2sr27ZtY9SoUZx22mkZ+RmIhGFGzYw+j/RJtG+pjRwKEmoycPdniV4d9NTmJ8BPwoyjm3cSdA4l2t4H7R/G8U444QSeeeYZHnnkEc477zy++93vst9++3HdddcBcNttt/HNb36To446ikceeYSTTz6Z2267DXfn/PPP5/rrr+/xvIny6Oc+9zlWrVrFo48+yj/90z9x0kkn8b3vfY+Ghgaam5sZNmwY1157rWoCREpUac5NNDxB51Ci7Rm0YcMGDj74YGbNmsWFF17I6tWrOeuss2hpaaGlpYXa2lrefvttjjjiCC699FJOP/10Xn75ZSZPnsyyZct49913Adi2bRsbNkRno41EIuzevRuIJpsHH3yQ1tZWPvzwQx544AGOP/54Nm3aREVFBeeeey5XXnklq1ev7vjgHzx4MDt27GDZsmWhv38pDtkq1Gp4pIHyueXYdUb53HIaHmkI5TxSRNNRpGTePKir63qrqKIiuj1kK1as4Mc//jGRSISBAwdy1113dWtz33330djYSCQS4ZBDDmHOnDkceOCB/OAHP+Ckk05i7969RCIRbrnlFiorK6mrq2P06NEceeSRNDU1MXPmTCZOnAjARRddxLhx43j88cf57ne/S79+/YhEIixatIgDDjiAWbNmUVNTQ1VVVcctKJGepFP4lYqGRxpY1Lyo43mbt3U8X3jqwoydR6Is7NvzYaitrfX4xW1effVVvvCFLyR/kKamaB/BO+9ErwjmzctYf4F0lfLvRvJa1YIqNmzvvkZK5aBK1l++PmPnKZ9bTpu3ddteZmXsmbMnY+cpJWa2yt1rg14rzSsDiH7w68NfJGXpFH6lIigR9LRd0lOafQYi0mfpFH6loszKUtou6VEyEJGUpFP4lYq68XUpbZf0KBmISEpm1Mxg8WmLqRxUiWFUDqpk8WmLMz5Wf+GpC6mvre+4EiizMupr69V5HJLS7UCWrNHvRiQ/9NSBrCsDEUlZsovEhLGYTLL7Z7pdroUdp64MMmjgwIHs2LEj8LVjjjkm1JlLe7Np0yYuvfTSPhWWTZo0ifnz51NbG/gHRa/y4XcjmRNfZwDRCd86LxID0L+sP+7O7r27e2xXEalI+jZT0LmD9s90u1zLVJy6MsihbExh3dmePcHjrw899NCsVRi3v2cpTskuEvNx28ddEkGidqksJpPs4jaZbpdr2YizZJNBiDNYpzWF9fbt26mqqmLv3r0AtLa2MmzYMHbv3s0f/vAHpk6dyvjx4zn++ON57bXXAJg5cyZXXHEFJ554IldddRVPP/00Y8eOZezYsYwbN44PPviA9evXU11dDUQ/rK+88kpqamoYPXo0N998MwBPPPEE48aNo6amhgsuuICPPvqo23u75557qKmpobq6mquuuqpj+8CBA5kzZw5HHXUUzz//fOZ+mJJ3wlj4JdljJlvjkOl2uZaNOEsyGTQ1RWej2LAhOn/1hg3R55lMCC+88ALz5s1j3bquyz3ffffdnHzyybS0tPDSSy91m7V00KBBjBkzhqeffhqAhx9+mJNPPplIJEJdXR0333wzq1atYv78+TQ0fDJPyxtvvMHy5cv5t3/7N+bPn88tt9xCS0sLK1euZJ999ulyjsWLF/PHP/6R3//+97z88svMmDGDXbt2MXPmTO677z7WrFnDnj17WLRoUZf9Nm3axFVXXcWTTz5JS0sLL774Ig8++CAQXS+hurqa3/3udxx33HEZ+ilKPgpj4Zdkj5lsjUOm2+VaNuIsyWTQ0wzWmdLTFNZLly7l2muvZc2aNey3337d2kyfPr1j8Zl7772X6dOns2PHDp577jnOOeecjsVtNm/e3LHPOeecQ1lZdAjescceyxVXXMFNN93Ee++9R3l510Lz5cuXc/HFF3dsP/DAA3n99dc5/PDD+dznPgfA+eefzzPPPNNlvxdffJFJkyYxZMgQysvLmTFjRkebsrIyvva1r/X1xyUFJNlFYvqX9SfSL9Jru1RqFJKtcch0u1zLRpwlmQyyMIN1r1NYDx06lPPOO4+77rqLBx54oOO2TnNzM6effjqPPfYY27ZtY9WqVXz5y19m7969HHDAAR2zm7a0tPDqq68Gnu/qq6/mtttuY+fOnXzxi1/suJ3Uzt0xs27betNTmwEDBnQkIyluQXUGS89cypIzlnTZtuSMJSw9c2mv7VLpBE22xiHT7XItK3EmWvUmnx/prnRWWdl1kbP2R2Vl0ocI1Hmls1NPPTXwtfXr1/vu3bvd3f3GG2/0yy67LPBYZ599tp977rleX1/fse3oo4/2n//85+7uvnfvXm9paXF39/PPP99/8YtfdLR76623Or4/44wz/IEHHvA//vGPPmrUKHd3X7RokX/ta1/riGPr1q2+c+dOHzZsmL/55psdx1ywYIG7u3/pS1/yF1980Tdt2uTDhw/3LVu2+J49e3zy5Mn+4IMPdnl/QbTSmUh+oIeVzkryymDevOiM1Z1laQZrVqxY0dGxe//993PZZZcFtps+fTqNjY1Mnz69Y1tTUxO33347Y8aMYdSoUfzqV78K3HfBggVUV1czZswY9tlnH0455ZQur1900UUMHz6c0aNHM2bMGO6++24GDBjA0qVLOeecc6ipqaFfv35cfPHFXfb7zGc+w/XXX8+JJ57ImDFjOPLIIznjjDPS/ImIJFZstQLx8inukq0z0AzW2aM6A+mLYqsViJeLuHuqMyjZZCDZo9+N9EWy6yZka32FTMtF3Co6E5GCU2y1AvHyLW4lAxHJS8VWKxAv3+JWMhCRvFRstQLx8i1uJQMRyUvFVisQL9/iVgeyhE6/G5H8oA7kLGmfjC7IMccck/bx58yZw/Lly1Pa56GHHuJf/uVfemyzadMmzj777HRCE0laKmscpLMeQj6N4c/HeOLpyiCDgtYzaGtrC32ahmycIx358LuR/BA0tj5o3YOKSAXnjzmfO1+6M6m2+V57kC/x6MogQJhZOqwprGfOnNmxJkFVVRVz587luOOO4xe/+AWPPvoon//85znuuOO49NJLmTZtGgB33HEHs2fPBqJTXV966aUcc8wxHHHEER3HSmZ667lz5zJhwgSqq6upq6tLai4jkXhB8/IHrXvQuruVxasWJ90239cpyLd4gpT33qT4xGfpDds3UPdwHUDGsvQLL7zAK6+80m3m0vYprK+55hra2tpojZs+tfMU1ieeeGKXKazjDRgwgGeffZZdu3YxYsQInnnmGQ4//HC+8Y1vJIxr8+bNPPvss7z22mucfvrp3W4PdZ7eury8nG3btgEwe/Zs5syZA8B5553Hr3/9a0477bQ+/WykdKUyhr7Nk18kKd9rD/ItniAleWWQjSyd6Smsg7Rvf+211zjiiCM6ztdTMjjzzDPp168fI0eO5C9/+Uu314OmtwZ46qmnOOqoo6ipqeHJJ59k7dq1Pb19kUCpjKEvs+RvfeZ77UG+xROkJJNBNrJ0pqew7ukcqdyy+dSnPtXxfdB+HjC99a5du2hoaGDZsmWsWbOGWbNmsWvXrqTPKdIuaGx90LoHFZEK6sbXJd0232sP8i2eICWZDHKZpTds2MDBBx/MrFmzuPDCC1m9ejVnnXVWxxoFtbW1DBw4kIkTJ3LZZZcxbdq0XjuHP//5z/P222+zfv16gI6rir446aSTuPXWWzvWUt62bVvHB//gwYPZsWNH1tZSluITNLY+aN2DxactZuGpC5Num++1B/kWT5CS7DOYN3leYM9+NrL0ihUr+PGPf0wkEmHgwIHcddddge2mT5/OOeecw4oVK3o95j777MPChQuZOnUqgwcPZuLEiX2O76KLLuKNN95g9OjRRCIRZs2axezZs5k1axY1NTVUVVUxYcKEPh9fZEbNjMAPwUTbkm2b7L65km/xxAt1aKmZDQPuAg4B9gKL3f3f49oY8O/AV4BWYKa7r+7puBmZwnpNE9c8cQ3vbH+H4YOGM2/yvLz+RfVmx44dDBw4EHfnkksuYcSIEfzDP/xDrsMCNLRUJF/kcmjpHuAf3f0LwBeBS8xsZFybU4ARsUcdsIgsmFEzg/WXr2fv9/ey/vL1BZ0IAH76058yduxYRo0axfbt2/n2t7+d65CkACVb5JVOMVixSed959PPLKtFZ2b2K+An7v6bTtv+A1jh7vfEnr8OTHL3zQkOk7dFZxJMv5vCkGxBWNC2SL8IZsbHbR93bCuEBWbSlU4xWb4tbpO1DmQzqwLGAb+Le2ko8KdOzzfGtqVMhVD5R7+TwpFsQVjQtt17d3dJBJB/RVVhSGeYer4VomUlGZjZQOB+4HJ3fz/+5YBdun2CmFmdmTWbWfOWLVu67TBgwAC2bt2qD5884u5s3bqVAQMG5DoUSUIYBVD5VFQVhnSGqedbIVroo4nMLEI0ETS5+y8DmmwEhnV6fhiwKb6Ruy8GFkP0NlH864cddhgbN24kKFFI7gwYMIDDDjss12FIEoYPGh64DGO6xyxmiX5mybzvdPYNQ6jJIDZS6HbgVXe/IUGzh4DZZnYvcBSwvaf+gkQikUhgxa+IJCdoyHW6fQb5VFQVhnSGqedyiHuQsG8THQucB3zZzFpij6+Y2cVmdnGszaPA28BbwE+BhpBjEpEAyRaEBW1beuZSlpyxJK+LqsKQTjFZvhWiFc0U1iIi0rO8GE0kIoUpaCx8wyMNlM8tx64zyueW0/BI8AV9Po2jT6QQYswGXRmISEJBY+HL+5WzZ++ebm3ra+tZeOrCHvfNt9qDQogxk3q6MlAyEJGEqhZUJT3CqMzK2DPnkySRaN/KQZWsv3x9pkJMSyHEmEm6TSQifZLOYjT5No4+SCHEmC1KBiKSUDqL0RTCgi6FEGO2KBmISEJBi7KU9wsuT6obX9frvvlWe1AIMWaLkoGIJBQ0Fv6OM++gvra+40qgzMq6dR4n2jffOmYLIcZsUQeyiEiJUAeyiHQx5YYG7Pvl2LWGfb+cKTdE6wRyVT+Q6LzJnifT7RIp5poEXRmIlJgpNzTwxPuLus4X7HBo+Ug2ta3r1j7s+oGGRxpY1Nx9TavJh0/m+Y3P93qeZONJN+5iqElQnYGIdLDvl0O/tu4vOIETyoddP1A+t7zbsNSexJ8n2XjSjbsYahJ0m0hEPmHJf/BC+PUDqSSCoPMkG0+6cRd7TYKSgUip8bLe23QSdv1A/PF7E3+eZONJN+5ir0lQMhApMZMH1XVfSzDWZxAk7PqB+ON3xHn45KTOk2w86cZd7DUJSgYiJWb5FQuZvH897C2LJoW9ZUzev54//++1OakfWHjqwsDzLv9fy5M6T7LxpBt3sdckqANZRKREqANZpAhla8x7UxNUVUG/ftGvTcUztF46CXUNZBEJR/yY9w3bN1D3cPTeeyZvWzQ1QV0dtMaG1m/YEH0OMKM47o5IjG4TiRSgbI15r6qKJoBu56mE9Zk7jWSJbhOJFJlsjXl/J8HhEm2XwqVkIFKAsjXmfXiCwyXaLoVLyUCkAGVrzPu8eVDR9TRUVES3S3FRMhApQNka8z5jBixeHO0jMIt+XbxYncfFSB3IIiIlQh3IIkUo2fH/YdQJ5FvtQTGvM5AtqjMQKUDJjv8Po04g32oPslVzUexSuk1kZscAVXRKIu5+V+bD6pluE0mpS3b8fxh1AvlWe1AM6wxkS0+3iZK+MjCznwGfBVqA9gnIHch6MhApdcmO/w+jTiDfag+KfZ2BbEnlNlEtMNILscdZpMgMHx7813n8+P9k24Vx7mwZPmh44JVBsawzkC2pdCC/AhwSViAikrxkx/+HUSeQb7UHxb7OQLb0mgzM7GEzewgYDKwzs8fN7KH2R/ghiki8ZMf/h1EnkG+1B8W+zkC29NqBbGZf6ul1d386oxElQR3IIiKpS6vOwN2fjn3gf6X9+87bejnxEjN718xeSfD6JDPbbmYtscecZN6QiIhkVip9Bn8XsO2UXva5A5jaS5uV7j429pibQjwiBS3dwq2hQ6O3adofQ4cGHzOV8zQ0QHl59Hjl5dHnQQVdKvIqPsncJqoHGoAjgD90emk/4Lfufm4v+1cBv3b36oDXJgFXuvu0VILWbSIpdPGFWxDthE323vvQobBpU+/tIpHoB/vHH/d+noYGWLQo7gA1TZSfVceefp8EGukXwcz4uO2Tg1ZEKnSfvgD0dJsomWQwCPg0cD1wdaeXPnD3bUmcvIqek8H9wEZgE9HEsLa3YyoZSKFLt3DLLL3zB52nvBza2uIaXl4FBwQEGnRMFXnlvXSLzsqA94FLAg58YDIJoQergUp332FmXwEeBEYENTSzOqAOYLgmU5cCl+vCraDzdEsEAIOSD0hFXoUtmT6DVUBz7OsW4A3gzdj3q9I5ubu/7+47Yt8/CkTMbHCCtovdvdbda4cMGZLOaUVyLteLxgSdp6wsoOH25ANSkVdhS2Y00eHufgTwOHCauw9294OAacAv0zm5mR1iFr3gNbOJsXi2pnNMkUKQbuHWoYcm1y4Sgf79kztP+2RzXTwxj/K9XQON9IvQv6zrQVXkVfhSGU00IfbXOwDu/hjQYw2Cmd0DPA/8rZltNLMLzexiM7s41uRs4BUzewm4Cfi6pruQUpBu4daf/9w9IRx6KDQ2dj3m0qWwZEly51m4EOrrP7lCKCuD+uNmcMfZXQu6lp65lCVnLFGRV5FJetZSM3scWAk0Ep2g7lzgBHc/ObzwgqkDWUQkdZla3OYbwBDgAaIdvQfHtomUrGwt8hI0/j/ZeAJrB/JscZpEVM+QPVr2UqSP0q0VSFbg+H+it3QWLuw5nvJy2LOn+77x28OIO13xi9aA6hnSlW6dwQJ3v9zMHiZ6e6gLdz89M2EmT8lA8kG2FnkJHP9P9J5+5w/0RPEkK1eL0ySiRWsyL906g5/Fvs7PXEgihS9btQKB4/8Dtqd73lwtTpOIFq3Jrl6Tgbu31xKUAf/t7q09tRcpFdla5KWsLPGVQTLxJCvfajm1aE12pdKBPBNoMbPnzexHZnaamX06pLhE8l62FnkJHP8fsD0onvIEf+7Fb8/l4jSJaNGaLHP3lB7AocClwDvAnlT3z8Rj/PjxLpIPGhvdKyvdzaJfGxvDOU99vXtZmTtEv9bXJx9P0L7ZijtdjS83euWNlW7XmlfeWOmNL+dpoAUCaPYEn6up1BmcCxwP1AB/BZ4lOv3082EkqZ6oA1lEJHWZqjNYAIwFfgpc6u4/ykUiEMmkMMbbT5nSdZ2BKVOCx/onqh1Iti4g3bULRDpLqc7AzEYBJwDHEZ1d9HV3Py+k2BLSlYFkQhh1AlOmwBNP9D2mkSNh3bru2+M7kfv3B3fYvfuTbamsXSClKa06g04H2R84luh8RMcDg4mOLjo/U4EmS8lAMiGMOoF01xkIQ77VD0jupFtn0O7ZTo+fuPvGTAQnkiu5XlMgW4rt/Ug4kk4G7j66p9fN7GZ3/076IYlkR7bqBHKt2N6PhCOVDuTeHJvBY4mELow6gcmT04tp5Mjg7fEFZv37R/sIOktl7QKReJlMBiIFJd01BYIsX949IUyeHLBOQH3wtrVrg7ffeWfXOJcsia5V0Ne1C0TiZWzWUjNb7e5HZuRgvVAHsohI6jJVZ9DreTJ4LJGCkM5Y/1RqAtKpH1DtgSQlUWlyqg9gZqaO1dtD01FIPmhsdK+oiE7z0P6IRNz79++6raKi+3QPQfsGtUu1bSb3leJDOtNRJFrHoFMy0XoGUpJSWT8gfqx/KjUO6dRDZGvNBSkM6dYZaB0DkQCpjN+Pb5tKjUM69RClUksh6UtmPYOnsxGISKFJZf2A+LH+qdQ4pFMPUSq1FJK+pDuQzWyEmS0zs3Vm9nb7I8zgRPJZUJ1CsmP9U6lxSKceIltrLkgRSNSZEP8gOg3FZOBloBK4Frgu2f0z+VAHsuSLoHUBkl0rIJU1BdJZf6BQ1i6Q8JGh9QxWuft4M1vj7jWxbSvd/fgwklRP1IEsIpK6TE1Ut8vM+gFvmtls4M/AwZkIUEREciuVorPLgQqiS16OB84Dsj59tRSfQimK0mIyUsxSno4itq6Bu/sH4YTUO90mKh5hLDAThqA4gxaYycfYRdplanGbWmApsF9s03bgAndflZEoU6BkUDwKpSgqnQIzkXyRqT6DJUCDu6+MHfQ4osmhx3UORHpSKEVR6RSYiRSCVPoMPmhPBADu/iyQs1tFUhwSFT/lW1FUKvHkW+wiyUglGbxgZv9hZpPM7EtmthBYYWZHmllWpq6W4lMoRVFBcQYtMJOPsYskI5XbRGNjX78ft/0YohPZfTkTAUlpae9oveaa6O2V4cOjH6b51gGbKM6gbfkWu0gyMra4TTapA1lEJHUZWdzGzP7GzG43s8diz0ea2YW97LPEzN41s1cSvG5mdpOZvWVmL+t2k/RFQwOUl0eXeiwvjz5Ppx1kfjEZ1SNI3ks0T0X8A3gM+HvgpdjzcmBNL/ucABwJvJLg9a/EjmvAF4HfJROL5iaSdvX1XRduaX/U1/etnXvmF5NJdsEbkbCRobmJXnT3CWb2e3cfF9vW4u5je9mvCvi1u1cHvPYfwAp3vyf2/HVgkrtv7umYuk0k7crLoa2t+/ayMtizJ/V2EM5iMkFUjyDZlqk1kD80s4OIrXpmZl8kWniWjqHAnzo93xjb1o2Z1ZlZs5k1b9myJc3TSrEI+oAP2p5sOwhnMZl024qELZVkcAXwEPBZM/stcBfwnTTPbwHbAi9V3H2xu9e6e+2QIUPSPK0Ui7Ky5LYn2w7Sq31QPYIUqlSSwWeBU4gOJX0ceJPUhqYG2QgM6/T8MGBTmseUElJXl9z2ZNtB5heTSXbBG5GcStSZEP8AXo59PQ54BjiDJDp8gSoSdyCfStcO5BeSiUUdyNJZfb17WVm0Y7asLLhTOJV27plfTEYLzEg+IEMdyL9393Fmdj3RUUR3d+5MTrDPPcAkYDDwF6IFa5FYErrVzAz4CTAVaAW+5e699gyrA1lEJHWZmqjuz7HRP1OAfzWzT9HLbSZ3/0YvrztwSQoxiIhICFLpM/h7on0FU939PeBA4LthBCUiItmV9JWBu7cCv+z0fDPQYz2AiIgUhlSuDEREpEgpGYiIiJKBiIgoGYiICEoGIiKCkoGIiKBkICIiKBmIiAhKBiIigpKBiIigZCAiIigZiIgISgYiIoKSgYiIoGQgIiIoGYiICEoGIiKCkoGIiKBkICIiKBmIiAhKBiIigpKBiIigZJAdTU1QVQX9+kW/NjXlOiIRkS7Kcx1A0Wtqgro6aG2NPt+wIfocYMaM3MUlItKJrgzCds01nySCdq2t0e0iInlCySBs77yT2nYRkRxQMgjb8OGpbRcRyQElg7DNmwcVFV23VVREt4uI5Aklg7DNmAGLF0NlJZhFvy5erM5jEckrGk2UDTNm6MNfRPJa6FcGZjbVzF43s7fM7OqA1yeZ2XYza4k95oQdU15Q7YGI5JFQrwzMrAy4Bfg7YCPwopk95O7r4pqudPdpYcaSV1R7ICJ5Juwrg4nAW+7+trt/DNwLnBHyOfOfag9EJM+EnQyGAn/q9HxjbFu8o83sJTN7zMxGBR3IzOrMrNnMmrds2RJGrNmj2gMRyTNhJwML2OZxz1cDle4+BrgZeDDoQO6+2N1r3b12yJAhmY0y21R7ICJ5JuxksBEY1un5YcCmzg3c/X133xH7/lEgYmaDQ44rt1R7ICJ5Juxk8CIwwswON7P+wNeBhzo3MLNDzMxi30+MxbQ15LhyS7UHIpJnQh1N5O57zGw28DhQBixx97VmdnHs9VuBs4F6M9sD7AS+7u7xt5KKj2oPRCSPhF5n4O6Puvvn3P2z7j4vtu3WWCLA3X/i7qPcfYy7f9Hdnws7pj5Jti5gypToX/vtjylTgvdNpc5ANQkiEjIrxD/Ca2trvbm5OXsnjK8LgOg9/vhbO1OmwBNPdN/fDDr/nPv3jz7fvbvn46VybhGRXpjZKnevDXxNySAJVVXRwrB4lZWwfv0nzy1o8FQK4o+XyrlFRHrRUzLQRHXJyFZdQNDxVJMgIlmgZJCMbNUFBB1PNQkikgVKBslIti5g8uTg/eNvH/XvD5FI78dL5dwiImlQMkhGsnUBy5d3TwiTJ8PPftZ13yVLYOnS5OoMVJMgIlmgDmQRkRKhDmQREemRkkGyGhqgvDx6q6a8PPo82QKzICokE5E8ottEyWhogEWLkmsbX2AWVCCmQjIRyQEVnaWrvBza2vq+f3yBmArJRCQH1GeQrnQSAXQvEFMhmYjkGSWDZJSVpbd/fIGYCslEJM8oGSSjfbH6ZMQXmAUViKmQTETyjJJBMhYuhPr6T64Qysqiz5MpMAvqFFYhmYjkGXUgi4iUCHUgQ2rj+oNqCkaN6lpTMGpUdI6hztv694dPf7rrtk9/GoYO7bpt6FAtbiMi+cXdC+4xfvx4T0ljo3tFhXu0AiD6qKiIbo9XX9+1XbYeieJJJXYRkR4AzZ7gc7U0bhOlMq4/3ZqCdGhxGxEJkW4TpTKuP1eJALS4jYjkTGkkg1TG9adbU5AOLW4jIjlSGskglXH9qdQUZJIWtxGRHCqNZJDKuP5ENQUjR3ZtN3Jk99XKIhE44ICu2w44AA49tOu2Qw+FxkYtbiMieaM0OpBFREQdyAmlO34/aP+gegQRkTxXnusAciZ+TYENGz7pL0jmFkzQ/uee273dunXRhLB2bWbiFhEJQeneJkp3/H6i/RMpwJ+ziBQX3SYKku74fY3zF5EiUrrJIN3x+xrnLyJFpHSTQbrj94P2TyR+WKqISJ4p3WSQ7vj9oP0bG4PrEdR5LCJ5rnQ7kEVESkxOO5DNbKqZvW5mb5nZ1QGvm5ndFHv9ZTM7MuyYRESkq1CTgZmVAbcApwAjgW+YWfwN9FOAEbFHHbAozJhERKS7sK8MJgJvufvb7v4xcC9wRlybM4C7Ymsv/DdwgJl9JuS4RESkk7CTwVDgT52eb4xtS7UNZlZnZs1m1rxly5aMByoiUsrCTgYWsC2+xzqZNrj7YnevdffaIUOGZCQ4ERGJCntuoo3AsE7PDwM29aFNF6tWrfqrmaUwF0QXg4G/9nHffFRM76eY3gvo/eSzYnovkPz7qUz0QtjJ4EVghJkdDvwZ+Drwzbg2DwGzzexe4Chgu7tv7umg7t7nSwMza040tKoQFdP7Kab3Ano/+ayY3gtk5v2EmgzcfY+ZzQYeB8qAJe6+1swujr1+K/Ao8BXgLaAV+FaYMYmISHehT2Ht7o8S/cDvvO3WTt87cEnYcYiISGKlOB3F4lwHkGHF9H6K6b2A3k8+K6b3Ahl4PwU5HYWIiGRWKV4ZiIhIHCUDEREpnWRgZkvM7F0zeyXXsaTLzIaZ2VNm9qqZrTWzy3IdUzrMbICZvWBmL8Xez3W5jildZlZmZr83s1/nOpZ0mdl6M1tjZi1mVvDTBZvZAWa2zMxei/0fOjrXMfWFmf1t7HfS/njfzC7v8/FKpc/AzE4AdhCdB6k61/GkIzZ302fcfbWZ7QesAs5093U5Dq1PzMyAfd19h5lFgGeBy2JzVRUkM7sCqAX2d/dpuY4nHWa2Hqh196Io0jKzO4GV7n6bmfUHKtz9vRyHlZbYpKB/Bo5y9z4V5JbMlYG7PwNsy3UcmeDum919dez7D4BXCZjPqVDEJincEXsaiT0K9q8UMzsMOBW4LdexSFdmtj9wAnA7gLt/XOiJIGYy8Ie+JgIooWRQrMysChgH/C7HoaQldlulBXgX+I27F/L7WQB8D9ib4zgyxYH/MrNVZlaX62DSdASwBVgau413m5ntm+ugMuDrwD3pHEDJoICZ2UDgfuByd38/1/Gkw93b3H0s0bmpJppZQd7KM7NpwLvuvirXsWTQse5+JNG1Ry6J3XItVOXAkcAidx8HfAh0W3SrkMRudZ0O/CKd4ygZFKjYvfX7gSZ3/2Wu48mU2CX7CmBqbiPps2OB02P32e8FvmxmjbkNKT3uvin29V3gAaLrlBSqjcDGTleey4gmh0J2CrDa3f+SzkGUDApQrMP1duBVd78h1/Gky8yGmNkBse/3AaYAr+U0qD5y939y98PcvYropfuT7n5ujsPqMzPbNzZIgdjtlJOAgh2R5+7/D/iTmf1tbNNkoCAHXnTyDdK8RQRZmJsoX5jZPcAkYLCZbQS+7+635zaqPjsWOA9YE7vPDvDPsXmgCtFngDtjIyL6AT9394Ifklkk/gZ4IPr3B+XA3e7+n7kNKW3fAZpit1fepoAnxzSzCuDvgG+nfaxSGVoqIiKJ6TaRiIgoGYiIiJKBiIigZCAiIigZiIgISgYiIoKSgUhKzGxST9NSm9lMM/tJCOedaWaHdnq+3swGZ/o8UrqUDEQKw0zg0N4aifRVyVQgS+mITZvwc6KT3pUB/xd4C7gBGAj8FZjp7pvNbAXQQnS+nf2BC9z9BTObSHT20X2AncC33P31FOMYAtwKDI9tutzdf2tm18a2HRH7usDdb4rt83+AGcCfYnGuAtYTXRuhycx2Au2LsXzHzE4jOuX3Oe5ekFN4SH7QlYEUo6nAJncfE1vI6D+Bm4Gz3X08sASY16n9vu5+DNAQew2icyOdEJvZcg7wwz7E8e/Aje4+AfgaXdc3+DxwMtEk9H0zi5hZbazdOOCrRBMA7r4MaAZmuPtYd98ZO8ZfY7OJLgKu7EN8Ih10ZSDFaA0w38z+Ffg18D9ANfCb2Bw7ZcDmTu3vgegCSGa2f2zSvP2Izpc0guh8/pE+xDEFGBk7J8D+7ZO+AY+4+0fAR2b2LtE5gI4DftX+YW9mD/dy/PbZalcRTR4ifaZkIEXH3d8ws/HAV4Drgd8Aa9090Vq38RN0OdFbS0+5+1mxBYRW9CGUfsDRnf6SByCWHD7qtKmN6P9FIzXtx2jfX6TPdJtIik5s1E2ruzcC84GjgCHtC5/HbsmM6rTL9Nj244Dt7r4dGER0TVmIdt72xX8BszvFNbaX9s8Cp5nZgNjCRad2eu0DolcrIqHQXxNSjGqAH5vZXmA3UA/sAW4ys0FE/90vANbG2v+PmT1HrAM5tu1HRG8TXQE82cc4LgVuMbOXY+d8Brg4UWN3f9HMHgJeAjYQ7SfYHnv5DuDWuA5kkYzRFNZS0mKjia509+ZcxwLRpUzdfUdsnvpngDp3X53ruKT46cpAJL8sNrORwADgTiUCyRZdGYj0gZl9C7gsbvNv3f2SXMQjki4lAxER0WgiERFRMhAREZQMREQEJQMREQH+Pznmdy7aDYTxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xiris = iris.iloc[:, :-1].values # Premières colonnes \n",
    "\n",
    "Yiris = iris.iloc[:,-1].values # Dernière colonne \n",
    "\n",
    "setosa = iris[\"class\"] == \"Iris-setosa\"\n",
    "versicolor = iris[\"class\"] == \"Iris-versicolor\"\n",
    "virginica = iris[\"class\"] == \"Iris-virginica\"\n",
    "\n",
    "plt.scatter(Xiris[setosa, 0], Xiris[setosa, 1], color=\"red\", label=\"Iris-setosa\")\n",
    "plt.scatter(Xiris[versicolor, 0], Xiris[versicolor, 1], color=\"blue\", label=\"Iris-versicolor\")\n",
    "plt.scatter(Xiris[virginica, 0], Xiris[virginica, 1], color=\"green\", label=\"Iris-virginica\")\n",
    "\n",
    "plt.xlabel(\"sepal_length\")\n",
    "plt.ylabel(\"sepal_width\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 30)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "Xiris_train, Xiris_test, Yiris_train, Yiris_test = train_test_split(Xiris, Yiris, test_size=0.2, random_state=0)  \n",
    "\n",
    "len(Xiris_train), len(Xiris_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. AdaGrad\n",
    "\n",
    "Nous avons entraîné deux modèles : \n",
    "- **DG** : modèle avec la déscente du gradient normale\n",
    "- **AdaGrad** : modèle avec AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc_iris = OneHotEncoder()\n",
    "Yiris_train_enc = enc_iris.fit_transform(np.array(Yiris_train).reshape(-1,1))\n",
    "\n",
    "Yiris_train_enc.toarray()[:4, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.1. Convergence\n",
    "\n",
    "Ici, on veut tester la convergence : rapidité et convergence finale (moins d'erreur = meilleur)\n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ?\n",
    "- Donner une hypothèse\n",
    "- Essayer de justifier cette hypothèse en se basant sur la formule de AdaGrad\n",
    "- Peut-on utiliser AdaGrad lorsque nous avons beaucoup de minimums locaux ? Pourquoi ?\n",
    "\n",
    "**Réponse**\n",
    "- On remarque que les deux méthodes convergent vers le même optimum mais à des vitesses de convergences différentes. En effet, la DG atteind l'optimum qui minimise la fonction coût au bout de la 500ème itération, or que Adagrad ne l'atteind même pas au bout de la 50000ème itération.\n",
    "- Hypothèse : la méthode d'Adagrad ne nous permet pas toujours d'obtenir de meilleurs résultats que la DG en terme de vitesse de convergence et de qualité de l'optimum trouvé.\n",
    "- Justification : Adagrad met à jour les taux d'apprentissages de la fonction de mise à jours des thétas à chaque itération en le divisant par le cumule des gradiants au carré,  il adapte le taux d'apprentissage aux paramètres : effectuant des mises à jour plus petites (c'est-à-dire de faibles taux d'apprentissage) pour les paramètres associés à des fonctionnalités fréquentes et des mises à jour plus importantes (c'est-à-dire des taux d'apprentissage élevés) pour paramètres associés à des fonctionnalités peu fréquentes .Ce qui fait que notre nouveau parametre d'apprentissage est d'autant plus petit à chaque itération, et c'est la raison pour laquelle l'algorithme Adagrad converge trés lentement dans ce cas-là.  \n",
    "- Oui il est préférable d'utiliser Adagrad lorsque nous avons beaucoup de minimum locaux. Justification : Le fait qu'Adagrad permette d'augmenter le taux d'apprentissage de maniére proportionnelle pour tous les thétas permet de se deplacer de maniére proportionnelle sur toutes les dimensions de la trajectoire et ainsi d'eviter les minimums locaux d'une meilleure maniére contrairement a la DG qui, a cause du taux d'apprentissage alpha qui est fixe, ne prends pas en compte le passif des thetas (le taux avec le quel nous avons deja mis a jour les thetas) du coup on ne se deplace pas de façon propotionnelles dans toutes les dimensions par conséquent il est plus facile de rester bloquer dans des optimums locaux. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_ITER = 500\n",
    "mxent_dg = MaxEnt()\n",
    "mxent_dg.entrainer(Xiris_train, Yiris_train_enc.toarray(), max_iter=MAX_ITER)\n",
    "mxent_adagrad = MaxEnt()\n",
    "mxent_adagrad.entrainer(Xiris_train, Yiris_train_enc.toarray(), adagrad=True, max_iter=MAX_ITER)\n",
    "\n",
    "mxent_dg2 = MaxEnt()\n",
    "mxent_dg2.entrainer(Xiris_train, Yiris_train_enc.toarray(), max_iter=50000)\n",
    "mxent_adagrad2 = MaxEnt()\n",
    "mxent_adagrad2.entrainer(Xiris_train, Yiris_train_enc.toarray(), adagrad=True, max_iter=50000)\n",
    "\n",
    "\n",
    "plt.plot(mxent_dg.couts, label = \"DG\")\n",
    "plt.plot(mxent_adagrad.couts, label = \"AdaGrad\")\n",
    "plt.legend()\n",
    "#plt.autoscale()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(mxent_dg2.couts, label = \"DG\")\n",
    "plt.plot(mxent_adagrad2.couts, label = \"AdaGrad\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.1. Performance (AdaGrad)\n",
    "\n",
    "Ici, on veut tester la performance sur le dataset de test.\n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous (Précision vs Recall (setosa); GD vs AdaGrad (micro-avg)) ?\n",
    "- Donner une hypothèse sur la convergence AdaGrad (peut-t-on converger) et comment l'améliorer\n",
    "- Essayer de justifier cette hypothèse (GD vs AdaGrad)\n",
    "\n",
    "**Réponse**\n",
    "- On  remarque que le rappel = 1 ce qui veut dire que toutes les fleurs qui sont réellement des setosa ont été correctement prédites (FN =0), quant à la précision = 0.92 <> 1 ceci veut dire que des fleures autres que setosa ont été prédites comme étant des setosa (FP <> 0, on a obtenu de fausses prédictions). Par ailleurs, étant donné la proprtion inégale entre les fleures qui sont des setosa = 11 et les fleurs qui ne le sont pas = 13+6 = 19, on ne peut pas se fier à la précision comme mesure de qualité du modèle mais plutôt au F1-score qui lui, est une mesure globale de la précision d'un modèle qui combine précision et rappel et qui prend en compte les ingalités des classes , dans cet exemple, le F1-score = 0.96 il est proche de 1, donc le modèle est bon. Pour ce qui est de la macro-avg, on remarque que celle-ci a des valeurs plus proche de 1 (pour les trois métriques) avec la méthode DG plutôt qu'avec Adagrad, ceci induit que le modèle obtenu avec la DG fait de meilleures prédictions que celui avec Adagrad.  \n",
    "- Hypothése : Adagrad peut converger cependant lorsque les données ne sont pas eparses (pas éprapillés) il risque donc de ne pas converger. Il pourrait etre amelioré en diminuant l'agressivité de la decroissance du taux d'apprentissage qui se fait de maniére monotone avec Adagrad.\n",
    "- Justification : Lorsque les données ne sont pas éparses, certaines caractéristiques sont plus prises que d'autres, elles sont par conséquent plus fréquentes et donc la somme des gradients carrés associée au théta de cette caractéristique augmente ce qui fait que le taux d'apprentissage diminue (car le dénominateur grandit) de façon monotone jusqu'à atteindre une valeur proche de 0 qui fait que le théta en question n'est plus mis à jour et stagne sur une certaine valeur sans converger (ce n'est pas la valeur optimale). La différence avec la DG est qu'elle ne prend pas en compte la fréquence d'appartition d'une caractéristique dans le taux d'apprentissage qui est fixé au départ, ce qui ne risque pas de faire stagner le théta sur une certaine valeur uniquement à cause de sa fréquence d'apparition. Adagrad a été amelioré par Adadelta qui, elle, est une méthode qui ne somme pas tous les gradiants passés mais cumule plutôt la moyenne de tous les gradiants carrés passés (le taux de croissance du dénominateur est déjà beaucoup moins important) et fixe un seuil maximal à ce cumul pour l'empêcher d'atteindre des valeurs trop grandes qui feront que le taux d'apprentissage devienne trop petit jusqu'à devenir insiginifiant et par conséquent faire stagner théta en une certaine valeur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descente du gradient\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       0.92      1.00      0.96        11\n",
      "Iris-versicolor       1.00      0.23      0.38        13\n",
      " Iris-virginica       0.40      1.00      0.57         6\n",
      "\n",
      "       accuracy                           0.67        30\n",
      "      macro avg       0.77      0.74      0.63        30\n",
      "   weighted avg       0.85      0.67      0.63        30\n",
      "\n",
      "AdaGrad\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       0.92      1.00      0.96        11\n",
      "Iris-versicolor       1.00      0.08      0.14        13\n",
      " Iris-virginica       0.35      1.00      0.52         6\n",
      "\n",
      "       accuracy                           0.60        30\n",
      "      macro avg       0.76      0.69      0.54        30\n",
      "   weighted avg       0.84      0.60      0.52        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Descente du gradient\")\n",
    "print(classification_report(Yiris_test, enc_iris.inverse_transform(mxent_dg.predire(Xiris_test)), target_names=enc_iris.categories_[0]))\n",
    "\n",
    "\n",
    "print(\"AdaGrad\")\n",
    "print(classification_report(Yiris_test, enc_iris.inverse_transform(mxent_adagrad.predire(Xiris_test)), target_names=enc_iris.categories_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3. One-vs-Rest OU One-vs-One\n",
    "\n",
    "Nous avons entrainé deux modèles : \n",
    "- **One-vs-Rest** : ici, trois sous-modèles binaires sont entraînés ; un pour chaque class. Chaque sous modèle détecte si l'échantillon appartient à sa classe ou non. Lors de la prédiction, on prend la classe avec le max de probabilité\n",
    "- **One-vs-One** : ici, un modèle de régression logistique multinomiale (maximum entropy) est entraîné pour séparer les trois classes\n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "On remarque que la performance de One-vs-One est meilleure que celle de One-vs-Rest\n",
    "- Pourquoi ? (en se basant sur la limite de décision et les paramètres)\n",
    "- Quel mécanisme de ces deux est affecté beaucoup plus par les valeurs aberrantes (les échantillons d'une classe qui peuvent se retrouver aux milieu d'une autre classe)\n",
    "\n",
    "**Réponse**\n",
    "- On remarque les résultats obtenus avec le OvO sont meilleures que ceux obtenus avec OvR si on se réfere aux différents parametres f1-score, recall et precision (plus ils sont proches de 1 meilleur est le modéle). On explique ça par le fait qu'avec OvR l'ensemble de données sur lequel chaque classificateur est formé devient déséquilibré (une classe vs deux classes) ce qui entraine de mauvaises  prédictions sur les données de test (le modéle a mal apprit les données) contrairement au OvO qui maintient l'equilibre initial du dataset et permet par conséquent de bien entrainer le modéle et d'obtenir de meilleurs résulats sur les données de test. Donc OvR permet d'entrainer moins de classificateurs, ce qui en fait une option plus rapide et donc préférée. D'autre part, OvO est moins susceptible de créer un déséquilibre dans l'ensemble de données en raison de la dominance dans des classes spécifiques.\n",
    "- Le mécanisme le moins affécté par les valeurs aberrantes est le OvO car c'est un modéle moins rapide mais plus précis que le OvR il permet par conséquent de mieux prédire les valeurs abérantes contrairement au OvR qui lui est trés sensible aux valeurs abérantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n",
      "One-vs-Rest\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        11\n",
      "Iris-versicolor       0.93      1.00      0.96        13\n",
      " Iris-virginica       1.00      0.83      0.91         6\n",
      "\n",
      "       accuracy                           0.97        30\n",
      "      macro avg       0.98      0.94      0.96        30\n",
      "   weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "One-vs-One\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        11\n",
      "Iris-versicolor       1.00      1.00      1.00        13\n",
      " Iris-virginica       1.00      1.00      1.00         6\n",
      "\n",
      "       accuracy                           1.00        30\n",
      "      macro avg       1.00      1.00      1.00        30\n",
      "   weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "one2rest = LogisticRegression(solver=\"lbfgs\", penalty=\"none\", multi_class=\"ovr\")\n",
    "one2rest.fit(Xiris_train, Yiris_train)\n",
    "\n",
    "one2one = LogisticRegression(solver=\"lbfgs\", penalty=\"none\", multi_class=\"multinomial\")\n",
    "one2one.fit(Xiris_train, Yiris_train)\n",
    "\n",
    "print(\"One-vs-Rest\")\n",
    "print(classification_report(Yiris_test, one2rest.predict(Xiris_test)))\n",
    "\n",
    "print(\"One-vs-One\")\n",
    "print(classification_report(Yiris_test, one2one.predict(Xiris_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonne chance\n"
     ]
    }
   ],
   "source": [
    "print(\"Bonne chance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
